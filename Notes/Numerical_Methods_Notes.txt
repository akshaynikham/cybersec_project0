Numerical Analysis? âœ”ï¸
	Numerical Analysis is the study of methods to find approximate solutions to mathematical problems using numbers and computers.

Why â€œapproximateâ€ solutions?
	âœ”ï¸ Computers cannot store infinite digits
	âœ”ï¸ Real numbers like Ï€, âˆš2, e are irrational
	âœ”ï¸ Rounding is unavoidable

**Numerical methods trade exactness for feasibility.

Why Numerical Methods Are Needed?

	Realâœ”ï¸world problems:
		âœ”ï¸ Engineering equations
		âœ”ï¸ Physics simulations
		âœ”ï¸ Machine learning
		âœ”ï¸ Control systems
		âœ”ï¸ Cybersecurity simulations (optimization, crypto, modeling)

**In C, the name of an array itself is a pointer to its first element.

Function Prototyping: 
	A function prototype is a declaration of a function that tells the compiler about the function's name, return type, and parameters before the 	function is actually defined (written out).

Loss of Significance:
	âœ”ï¸ also called "Catastrophic Cancellation"
	âœ”ï¸ is a phenomenon in floatingâœ”ï¸point math where the precision of your result suddenly drops, often leaving you with a number that is mostly "garbage" or "noise."
	âœ”ï¸ It typically happens in two specific scenarios: when you subtract two numbers that are very close in value, or when you add/subtract numbers of wildly different scales.
		âœ”ï¸ The "Subtraction" Trap (Catastrophic Cancellation)
		âœ”ï¸ The "Scale" Trap (Absorption)

Machine Epsilon: 
	âœ”ï¸ The smallest difference between 1.0 and the next representable floatingâœ”ï¸point number. 
	âœ”ï¸ It represents the "rounding unit" or the maximum relative error of a computer's number system. If a change is smaller than Epsilon, the computer "ignores" it.

Interpolation:
	âœ”ï¸ interpolation is estimating unknown data points between known values by fitting a simple function (like a line or curve) through the given points, effectively "filling the gaps" in a dataset to 		predict intermediate values, unlike extrapolation, which guesses values outside the range.
	âœ”ï¸ Curve passes exactly through all data points
	âœ”ï¸ Used when data is accurate

Approximation:
	âœ”ï¸ Curve does not necessarily pass through all points
	âœ”ï¸ Used when data has errors/noise

Lagrange Interpolation:
	âœ”ï¸ Intuition: Imagine you have a few scattered points on a graph, and you want to find a smooth curve (a polynomial) that passes exactly through every single one of them.
	âœ”ï¸ Lagrange Interpolation is the formula that allows you to "construct" that curve.
	âœ”ï¸ Core Idea:
		âœ”ï¸ To find one big polynomial that fits all n points, let's create n smaller 'helper' polynomials. Each helper will be responsible for exactly one point
	âœ”ï¸ How it works:
		âœ”ï¸ Suppose we have three points: (x_0, y_0), (x_1, y_1), and (x_2, y_2).
		âœ”ï¸ Step 1:  The "Helper" Polynomials (L_i):
			âœ”ï¸ We create a special function for each x value. These are called Lagrange Basis Polynomials.
			âœ”ï¸ A helper function L_0 has a very specific job:
				âœ”ï¸ It must equal 1 when x = x_0.
				âœ”ï¸ It must equal 0 when x is any other given point (x_1 or x_2).
		âœ”ï¸ Step 2: Building the switch:
			âœ”ï¸ Build formula that makes x_1 and x_2 zero. 
				(x âœ”ï¸ x_1)(x âœ”ï¸ x_2)
		âœ”ï¸ Step 3: Now, to make sure it equals 1 when x = x_0, we divide it by those same values but with x_0 plugged in:
			L_0(x) = (x âœ”ï¸ x_1)(x âœ”ï¸ x_2)/(x_0 âœ”ï¸ x_1)(x_0 âœ”ï¸ x_2)
		âœ”ï¸ Step 3: Putting it all together:
			âœ”ï¸ Once we have all our helpers (L_0, L_1, L_2), we multiply each one by its corresponding y value and add them up:
				P(x) = y_0L_0(x) + y_1L_1(x) + y_2L_2(x)
	âœ”ï¸ Pros & Cons:
		
		Pros								Cons
		Simple to code: No complex matrix math needed.			Computationally Expensive: It takes O(n^2) time.
		Exact: It passes exactly through every point.			Wiggle Problem (Runge's Phenomenon): With too many points, the edges of the curve start to "wiggle" wildly.
		Direct: You don't need to solve for coefficients first.		Hard to update: Adding one new point requires recalculating everything.



Newtonâ€™s Forward Interpolation:
	âœ”ï¸ Newtonâ€™s method:
		Uses finite differences
		Easier to compute
	âœ”ï¸ Use Case Scenario:
		Data points are equally spaced
		Interpolation point is near the beginning of table
	âœ”ï¸ Advantages & Limitations
		Advantages
			âœ”ï¸ Efficient
			âœ”ï¸ Easy to update
			âœ”ï¸ Uses difference table

		Limitations
			âœ”ï¸ Equal spacing required
			âœ”ï¸ Accuracy reduces far from xâ‚€

	âœ”ï¸ It is based on the idea of "Finite Differences"â€”looking at how the 1y values change stepâœ”ï¸byâœ”ï¸step.
	âœ”ï¸ The Core Concept: The Difference Table
		âœ”ï¸ First Difference (â–³ y_0): y_1 âœ”ï¸ y_0
		âœ”ï¸ Second Difference (â–³^2 y_0): â–³ y_1 âœ”ï¸ â–³ y_0
	âœ”ï¸ The Formula:
		âœ”ï¸ y_x = y_0 + p * â–³ y_0 + p(pâœ”ï¸1)/2! * â–³^2 y_0 + p(pâœ”ï¸1)(pâœ”ï¸2)/3! * â–³^3 y_0 + ....
	âœ”ï¸ What is p?
		âœ”ï¸ In Newton's method, we don't use x directly. We use a "normalized" distance called p (sometimes called u). It tells us how far we are from the start point (x_0) 				in terms of "steps": p = x âœ”ï¸ x_0/h
		âœ”ï¸ Where h is the fixed interval between points, like 1 or 10
	âœ”ï¸ constant difference [ the secret decoder]:
		âœ”ï¸ The computer doesn't know what your curve looks like. It just sees the numbers 1, 3, 6, 10.

		âœ”ï¸ By doing these subtractions, the computer discovers:
			âœ”ï¸ The Shape: "It took 2 steps to find a constant, so I need an x^2 formula."
			âœ”ï¸ The Growth: "The constant is 1, so I know exactly how much to bend the curve."
	âœ”ï¸ The "Secret" Summary:
		âœ”ï¸ If the 1st subtraction is constant âœ”ï¸> The data is a Straight Line.
		âœ”ï¸ If the 2nd subtraction is constant âœ”ï¸> The data is a Simple Curve (Parabola).
		âœ”ï¸ If the 3rd subtraction is constant âœ”ï¸> The data is a Complex Curve (Sâœ”ï¸shape).
	âœ”ï¸  Why p(pâœ”ï¸1), p(pâœ”ï¸1)(pâœ”ï¸2), etc.?
		âœ”ï¸ they make sure the formula hits the data points exactly.
		âœ”ï¸ Think of them as "Safety Switches":At the 1st point (p=0):The term p becomes 0. This "shuts off" the Speed and the Bend, leaving you with only the Height.
		âœ”ï¸ At the 2nd point (p=1):The term (pâœ”ï¸1) becomes (1âœ”ï¸1) = 0. This "shuts off" the Bend, leaving you with just Height + Speed.
		âœ”ï¸ At the 3rd point (p=2):The term (pâœ”ï¸2) would become 0, shutting off the next layer of complexity. 


Newtonâ€™s backward Interpolation:
	âœ”ï¸ If Newton Forward Interpolation is about starting at the beginning and looking ahead, Newton Backward Interpolation is about starting at the very end of your data and looking 		back.
	âœ”ï¸ We use this when we want to estimate a value near the end of a table
	âœ”ï¸ Ex: if you have data for 2020, 2021, 2022, 2023, and 2024, and you want to predict something for late 2024
	âœ”ï¸ Key difference:
		Feature,	Forward Difference,		Backward Difference
		Starting Point	Uses x0â€‹ (First point),		Uses xnâ€‹ (Last point)
		Direction,	Looks forward (Î”),		Looks backward (âˆ‡)
		The "P" Formula	p=(xâˆ’x0â€‹)/h,			p=(xâˆ’xnâ€‹)/h
		The "P" Product	p(pâˆ’1)(pâˆ’2)â€¦,			p(p+1)(p+2)â€¦
	âœ”ï¸ The Backward Difference Table:
		âœ”ï¸ The table is built exactly the same way (by subtracting), but we focus on the bottom diagonal instead of the top.
		âœ”ï¸ The symbol for backward difference is â–½ (Nabla)
			âœ”ï¸	â–½y_n = y_n âœ”ï¸ y_nâœ”ï¸1 
			âœ”ï¸	â–½^2y_n = â–½y_n âœ”ï¸ â–½y_nâœ”ï¸1
	âœ”ï¸ The Formula:
		Because we are moving "backward" into the past relative to the last point, the signs in the formula change to plus:
		y = y_n + pâ–½y_n + p(p+1)/2! â–½^2y_n + p(p+1)(p+2)/3! â–½^3y_n + ...
	
	âœ”ï¸ Visual Intuition: 
		âœ”ï¸ The "Endâœ”ï¸Point Anchor"Imagine you are at the finish line of a race and you want to guess where a runner was 2 seconds ago.
		âœ”ï¸ Start: You start at the Finish Line height (y_n).
		âœ”ï¸ Speed: you use the runner's final speed (â–½y_n) to project backward.
		âœ”ï¸ Bend: You use the final acceleration (â–½^2y_n) to curve the path.


Newton's Divided Difference:
	âœ”ï¸ Forward and Backward methods require the x values to be perfectly spaced (like 1, 2, 3)
	âœ”ï¸ Divided Difference works for any x values (like 1, 5, 12, 100)
	âœ”ï¸ The Core Idea: "Slope of Slopes":
		âœ”ï¸ In the Forward method, we just subtracted y values
		âœ”ï¸ In Divided Difference, we calculate the actual slope (the rate of change) between points.
		âœ”ï¸ The notation looks like this: f[x_0, x_1]. This is just mathâœ”ï¸speak for:
				y_1 âœ”ï¸ y_0
				âœ”ï¸âœ”ï¸âœ”ï¸âœ”ï¸âœ”ï¸âœ”ï¸âœ”ï¸âœ”ï¸âœ”ï¸âœ”ï¸âœ”ï¸
				x_1 âœ”ï¸ x_0
		âœ”ï¸ Itâ€™s essentially: 	Change in Height
					âœ”ï¸âœ”ï¸âœ”ï¸âœ”ï¸âœ”ï¸âœ”ï¸âœ”ï¸âœ”ï¸âœ”ï¸âœ”ï¸âœ”ï¸âœ”ï¸âœ”ï¸âœ”ï¸âœ”ï¸âœ”ï¸âœ”ï¸âœ”ï¸
					Change in Distance
	âœ”ï¸ The Divided Difference Table:
		âœ”ï¸ How to calculate the layers:
		âœ”ï¸ 1st Difference: y_1 âœ”ï¸ y_0 / x_1 âœ”ï¸ x_0
		âœ”ï¸ 2nd Difference: (Result_2 âœ”ï¸ Result_1) / (x_2 âœ”ï¸ x_0) (we divide by the distance between x_2 and x_0)4
		âœ”ï¸ 3rd Difference: (Result_B âœ”ï¸ Result_A) / (x_3 âœ”ï¸ x_0)
	
	âœ”ï¸ The Formula:
		âœ”ï¸ Instead of a "step counter," we use the actual x distances:
		âœ”ï¸ P(x) = a_0 + a_1(x âœ”ï¸ x_0) + a_2(x âœ”ï¸ x_0)(x âœ”ï¸ x_1) + a_3(x âœ”ï¸ x_0)(x âœ”ï¸ x_1)(x âœ”ï¸ x_2) ....
		âœ”ï¸ a_0 = y_0 (The Height)
		âœ”ï¸ a_1 = f[x_0, x_1] (The Speed)
		âœ”ï¸ a_2 = f[x_0, x_1, x_2] (The Bend)

Errors in Interpolation & Numerical Errors:

	âœ”ï¸ Why Error Analysis Is Needed? 
		âœ”ï¸ Numerical methods give approximate answers, not exact ones.
	âœ”ï¸ So we must answer:
		âœ”ï¸ How close is the answer?
		âœ”ï¸ How much error is acceptable?
		âœ”ï¸ Will error grow with computation?
	âœ”ï¸ This is the core idea of numerical analysis

	âœ”ï¸ Types of Errors:
		âœ”ï¸ Absolute Error
			Absolute Error = âˆ£ True value âˆ’ Approximate value âˆ£
			âœ”ï¸ Example: 
				True = 3.14159
				Approx = 3.14
				Absolute error = 0.00159
		âœ”ï¸ Relative Error
			Relative Error = Absolute Error
					âœ”ï¸âœ”ï¸âœ”ï¸âœ”ï¸âœ”ï¸âœ”ï¸âœ”ï¸âœ”ï¸âœ”ï¸âœ”ï¸âœ”ï¸âœ”ï¸âœ”ï¸âœ”ï¸âœ”ï¸
					True value
		âœ”ï¸ Percentage Error
			Percentage Error = Relative Error Ã— 100

	âœ”ï¸ Errors in Interpolation:
		âœ”ï¸ Interpolation is the process of estimating a value between known data points. Even if your math is perfect, the "shape" of the formula you choose usually won't perfectly 			match the realâœ”ï¸world function.
	â€‹	âœ”ï¸ Why Interpolation Errors Happen?
			âœ”ï¸ Data Uncertainty: If your starting data points (x_i, y_i) are slightly wrong (due to measurement error), the resulting curve will be off.
			âœ”ï¸ Oscillations (Runge's Phenomenon): Using a very highâœ”ï¸degree polynomial to fit many points often causes the curve to wiggle wildly near the edges, leading to 					massive errors.
			âœ”ï¸ Distance from Nodes: The error is usually zero at the data points themselves, but it grows as you move further away from them.
	
	âœ”ï¸ Numerical Errors:
		âœ”ï¸ Numerical errors generally fall into two categories: how computers store numbers and how we simplify math
		âœ”ï¸ Roundâœ”ï¸off Errors:
			âœ”ï¸ These occur because computers have finite memory. They cannot store an infinite string of digits (like pi or 1/3).
			âœ”ï¸ Cause: A computer typically uses 32 or 64 bits to represent a number. Any digits beyond that are either chopped (ignored) or rounded.
			âœ”ï¸ Example: 	If a system only stores 4 decimal places, 1/3 approx 0.3333.
					The "lost" 0.0000333... is the roundâœ”ï¸off error
		âœ”ï¸ Truncation Errors:
			âœ”ï¸ These are "mathematical" errors. 
			âœ”ï¸ They occur when we replace an infinite process with a finite one to make it solvable.
			âœ”ï¸ Cause: Most commonly found when using a Taylor Series expansion but stopping after a few terms.
			âœ”ï¸ Example: 	The exact value of e^x is an infinite sum. 
					If we only use the first three terms (1 + x + x^2/2), we "truncate" the rest. 
					That missing part is the truncation error.
	
	âœ”ï¸ Error Propagation:
		âœ”ï¸ Small errors grow during calculations.
		âœ”ï¸ Example: 	ğ‘§=ğ‘¥âˆ’ğ‘¦
		âœ”ï¸  If x â‰ˆ y, then:
			âœ”ï¸ Large relative error
			âœ”ï¸ Loss of significance
	

Why Polynomial Interpolation Fails?
	âœ”ï¸ The Problem with Highâœ”ï¸Degree Polynomials
		âœ”ï¸ When: 










