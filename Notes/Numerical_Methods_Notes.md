Numerical Analysis? -
    Numerical Analysis is the study of methods to find approximate solutions to mathematical problems using numbers and computers.

Why ‚Äúapproximate‚Äù solutions?
    - Computers cannot store infinite digits
    - Real numbers like œÄ, ‚àö2, e are irrational
    - Rounding is unavoidable

#### Numerical methods trade exactness for feasibility.

Why Numerical Methods Are Needed?

    Real-world problems:
        - Engineering equations
        - Physics simulations
        - Machine learning
        - Control systems
        - Cybersecurity simulations (optimization, crypto, modeling)

In C, the name of an array itself is a pointer to its first element.

###### **Function Prototyping:**

A function prototype is a declaration of a function that tells the compiler about the function's name, return type, and parameters before the     function is actually defined (written out).

Loss of Significance:
    - also called "Catastrophic Cancellation"
    - is a phenomenon in floating-point math where the precision of your result suddenly drops, often leaving you with a number that is mostly "garbage" or "noise."
    - It typically happens in two specific scenarios: when you subtract two numbers that are very close in value, or when you add/subtract numbers of wildly different scales.
     - The "Subtraction" Trap (Catastrophic Cancellation)
     - The "Scale" Trap (Absorption)

**Machine Epsilon:**
    - The smallest difference between 1.0 and the next representable floating-point number. 
    - It represents the "rounding unit" or the maximum relative error of a computer's number system. If a change is smaller than Epsilon, the computer "ignores" it.

**Interpolation:**
    - interpolation is estimating unknown data points between known values by fitting a simple function (like a line or curve) through the given points, effectively "filling the gaps" in a dataset to         predict intermediate values, unlike extrapolation, which guesses values outside the range.
    - Curve passes exactly through all data points
    - Used when data is accurate

**Approximation:**
    - Curve does not necessarily pass through all points
    - Used when data has errors/noise

**Lagrange Interpolation:**
    - Intuition: Imagine you have a few scattered points on a graph, and you want to find a smooth curve (a polynomial) that passes exactly through every single one of them.
    - Lagrange Interpolation is the formula that allows you to "construct" that curve.
    - Core Idea:
        - To find one big polynomial that fits all n points, let's create n smaller 'helper' polynomials. Each helper will be responsible for exactly one point
    - How it works:
        - Suppose we have three points: (x_0, y_0), (x_1, y_1), and (x_2, y_2).
        - Step 1:  The "Helper" Polynomials (L_i):
            - We create a special function for each x value. These are called Lagrange Basis Polynomials.
            - A helper function L_0 has a very specific job:
                - It must equal 1 when x = x_0.
                - It must equal 0 when x is any other given point (x_1 or x_2).
        - Step 2: Building the switch:
            - Build formula that makes x_1 and x_2 zero. 
                (x - x_1)(x - x_2)
        - Step 3: Now, to make sure it equals 1 when x = x_0, we divide it by those same values but with x_0 plugged in:
            L_0(x) = (x - x_1)(x - x_2)/(x_0 - x_1)(x_0 - x_2)
        - Step 3: Putting it all together:
            - Once we have all our helpers (L_0, L_1, L_2), we multiply each one by its corresponding y value and add them up:
                P(x) = y_0L_0(x) + y_1L_1(x) + y_2L_2(x)

- Pros & Cons:
  
        Pros                                Cons
        Simple to code: No complex matrix math needed.            Computationally Expensive: It takes O(n^2) time.
        Exact: It passes exactly through every point.            Wiggle Problem (Runge's Phenomenon): With too many points, the edges of the curve start to "wiggle" wildly.
        Direct: You don't need to solve for coefficients first.        Hard to update: Adding one new point requires recalculating everything.

| **Pros**                                               | **Cons**                                                                                                    |
| ------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------- |
| Simple to code: No complex matrix math needed.         | Computationally Expensive: It takes O(n^2) time.                                                            |
| Exact: It passes exactly through every point.          | Wiggle Problem (Runge's Phenomenon): With too many points, the edges of the curve start to "wiggle" wildly. |
| Direct: You don't need to solve for coefficients first | Hard to update: Adding one new point requires recalculating everything.                                     |

**Newton‚Äôs Forward Interpolation:**

    - Newton‚Äôs method:
        Uses finite differences
        Easier to compute
    - Use Case Scenario:
        Data points are equally spaced
        Interpolation point is near the beginning of table
    - Advantages & Limitations
        Advantages
            - Efficient
            - Easy to update
            - Uses difference table
    
        Limitations
            - Equal spacing required
            - Accuracy reduces far from x‚ÇÄ
    
    - It is based on the idea of "Finite Differences"‚Äîlooking at how the 1y values change step-by-step.
    - The Core Concept: The Difference Table
        - First Difference (‚ñ≥ y_0): y_1 - y_0
        - Second Difference (‚ñ≥^2 y_0): ‚ñ≥ y_1 - ‚ñ≥ y_0
    - The Formula:
        - y_x = y_0 + p * ‚ñ≥ y_0 + p(p-1)/2! * ‚ñ≥^2 y_0 + p(p-1)(p-2)/3! * ‚ñ≥^3 y_0 + ....
    - What is p?
        - In Newton's method, we don't use x directly. We use a "normalized" distance called p (sometimes called u). It tells us how far we are from the start point (x_0)                 in terms of "steps": p = x - x_0/h
        - Where h is the fixed interval between points, like 1 or 10
    - constant difference [ the secret decoder]:
        - The computer doesn't know what your curve looks like. It just sees the numbers 1, 3, 6, 10.
    
        - By doing these subtractions, the computer discovers:
            - The Shape: "It took 2 steps to find a constant, so I need an x^2 formula."
            - The Growth: "The constant is 1, so I know exactly how much to bend the curve."
    - The "Secret" Summary:
        - If the 1st subtraction is constant -> The data is a Straight Line.
        - If the 2nd subtraction is constant -> The data is a Simple Curve (Parabola).
        - If the 3rd subtraction is constant -> The data is a Complex Curve (S-shape).
    -  Why p(p-1), p(p-1)(p-2), etc.?
        - they make sure the formula hits the data points exactly.
        - Think of them as "Safety Switches":At the 1st point (p=0):The term p becomes 0. This "shuts off" the Speed and the Bend, leaving you with only the Height.
        - At the 2nd point (p=1):The term (p-1) becomes (1-1) = 0. This "shuts off" the Bend, leaving you with just Height + Speed.
        - At the 3rd point (p=2):The term (p-2) would become 0, shutting off the next layer of complexity. 

Newton‚Äôs backward Interpolation:
    - If Newton Forward Interpolation is about starting at the beginning and looking ahead, Newton Backward Interpolation is about starting at the very end of your data and looking         back.
    - We use this when we want to estimate a value near the end of a table
    - Ex: if you have data for 2020, 2021, 2022, 2023, and 2024, and you want to predict something for late 2024

- Key difference:
  
          Feature,    Forward Difference,        Backward Difference
          Starting Point    Uses x0‚Äã (First point),        Uses xn‚Äã (Last point)
          Direction,    Looks forward (Œî),        Looks backward (‚àá)
          The "P" Formula    p=(x‚àíx0‚Äã)/h,            p=(x‚àíxn‚Äã)/h
          The "P" Product    p(p‚àí1)(p‚àí2)‚Ä¶,            p(p+1)(p+2)‚Ä¶

- The Backward Difference Table:
  
          - The table is built exactly the same way (by subtracting), but we focus on the bottom diagonal instead of the top.
          - The symbol for backward difference is ‚ñΩ (Nabla)
              -    ‚ñΩy_n = y_n - y_n-1 
              -    ‚ñΩ^2y_n = ‚ñΩy_n - ‚ñΩy_n-1
      - The Formula:
          Because we are moving "backward" into the past relative to the last point, the signs in the formula change to plus:
          y = y_n + p‚ñΩy_n + p(p+1)/2! ‚ñΩ^2y_n + p(p+1)(p+2)/3! ‚ñΩ^3y_n + ...
  
  - Visual Intuition: 
    - The "End-Point Anchor"Imagine you are at the finish line of a race and you want to guess where a runner was 2 seconds ago.
    - Start: You start at the Finish Line height (y_n).
    - Speed: you use the runner's final speed (‚ñΩy_n) to project backward.
    - Bend: You use the final acceleration (‚ñΩ^2y_n) to curve the path.

**Newton's Divided Difference:**
    - Forward and Backward methods require the x values to be perfectly spaced (like 1, 2, 3)
    - Divided Difference works for any x values (like 1, 5, 12, 100)
    - The Core Idea: "Slope of Slopes":
        - In the Forward method, we just subtracted y values
        - In Divided Difference, we calculate the actual slope (the rate of change) between points.
        - The notation looks like this: f[x_0, x_1]. This is just math-speak for:
                y_1 - y_0
                -----------
                x_1 - x_0

        - It‚Äôs essentially:     Change in Height
                    ------------------
                    Change in Distance
    
    - The Divided Difference Table:
        - How to calculate the layers:
        - 1st Difference: y_1 - y_0 / x_1 - x_0
        - 2nd Difference: (Result_2 - Result_1) / (x_2 - x_0) (we divide by the distance between x_2 and x_0)4
        - 3rd Difference: (Result_B - Result_A) / (x_3 - x_0)
    
    - The Formula:
        - Instead of a "step counter," we use the actual x distances:
        - P(x) = a_0 + a_1(x - x_0) + a_2(x - x_0)(x - x_1) + a_3(x - x_0)(x - x_1)(x - x_2) ....
        - a_0 = y_0 (The Height)
        - a_1 = f[x_0, x_1] (The Speed)
        - a_2 = f[x_0, x_1, x_2] (The Bend)

Errors in Interpolation & Numerical Errors:

    - Why Error Analysis Is Needed? 
        - Numerical methods give approximate answers, not exact ones.
    - So we must answer:
        - How close is the answer?
        - How much error is acceptable?
        - Will error grow with computation?
    - This is the core idea of numerical analysis
    
    - Types of Errors:
        - Absolute Error
            Absolute Error = ‚à£ True value ‚àí Approximate value ‚à£
            - Example: 
                True = 3.14159
                Approx = 3.14
                Absolute error = 0.00159
        - Relative Error
            Relative Error = Absolute Error
                    ---------------
                    True value
        - Percentage Error
            Percentage Error = Relative Error √ó 100
    
    - Errors in Interpolation:
        - Interpolation is the process of estimating a value between known data points. Even if your math is perfect, the "shape" of the formula you choose usually won't perfectly             match the real-world function.
    ‚Äã    - Why Interpolation Errors Happen?
            - Data Uncertainty: If your starting data points (x_i, y_i) are slightly wrong (due to measurement error), the resulting curve will be off.
            - Oscillations (Runge's Phenomenon): Using a very high-degree polynomial to fit many points often causes the curve to wiggle wildly near the edges, leading to                     massive errors.
            - Distance from Nodes: The error is usually zero at the data points themselves, but it grows as you move further away from them.
    
    - Numerical Errors:
        - Numerical errors generally fall into two categories: how computers store numbers and how we simplify math
        - Round-off Errors:
            - These occur because computers have finite memory. They cannot store an infinite string of digits (like pi or 1/3).
            - Cause: A computer typically uses 32 or 64 bits to represent a number. Any digits beyond that are either chopped (ignored) or rounded.
            - Example:     If a system only stores 4 decimal places, 1/3 approx 0.3333.
                    The "lost" 0.0000333... is the round-off error
        - Truncation Errors:
            - These are "mathematical" errors. 
            - They occur when we replace an infinite process with a finite one to make it solvable.
            - Cause: Most commonly found when using a Taylor Series expansion but stopping after a few terms.
            - Example:     The exact value of e^x is an infinite sum. 
                    If we only use the first three terms (1 + x + x^2/2), we "truncate" the rest. 
                    That missing part is the truncation error.
    
    - Error Propagation:
        - Small errors grow during calculations.
        - Example:     ùëß=ùë•‚àíùë¶
        -  If x ‚âà y, then:
            - Large relative error
            - Loss of significance

Why Polynomial Interpolation Fails?

    - The Problem with High-Degree Polynomials
        - When:
            - Number of data points increases
            - Degree of polynomial increases
            - Polynomial interpolation becomes unstable
            - This is called the Runge Phenomenon.
    
    - Imagine we have a set of data points (dots on a graph), and you want to draw a single, smooth curve that passes exactly through every single one of them. 
    - A common way to do this is by using a polynomial, which looks like this: P(x) = a_n x^n + a_{n-1} x^{n-1} + ... + a_1 x + a_0
    
    - The "failure" happens when we try to be too precise by adding too many points.  Here is the breakdown of why this goes wrong.
    
        - The "Wiggle" Problem (Runge's Phenomenon):
            -  In theory, adding more points should make the curve more accurate. However, in polynomial interpolation, the opposite often happens.
            -  As you increase the number of points, you increase the degree (the highest power of x) of the polynomial.
            -  High-degree polynomials are incredibly "springy" or "nervous."
            - To hit every single point exactly, the curve starts to oscillate‚Äîor wiggle‚Äîviolently, especially near the beginning and the end of the data set.
    
        - The Problem with "Equally Spaced" Points:
            - Most of us naturally want to put our data points at equal intervals (e.g., at x = 1, 2, 3, 4). Surprisingly, this is actually the worst way to do polynomial                     interpolation.
            - When points are equally spaced, the "errors" or wiggles at the edges of the interval grow exponentially as you add more points. Instead of the curve getting                     closer to the "true" shape, it flies off the top or bottom of the graph.
    
        - How do we fix it?
            - Because "simple" polynomial interpolation often fails for large data sets, mathematicians use two main workarounds:
                - Splines (The "Connect-the-Dots" Method): Instead of using one giant, complex equation for the whole graph, we use many small, simple equations (usually                     cubics) and "glue" them together at the points. This is how almost all modern computer graphics and fonts are drawn.
                - Chebyshev Nodes: If you must use one single polynomial, you can avoid the wiggles by clustering your data points more densely at the edges and further                     apart in the middle.
    
    - 

This is a great moment to pause and categorize the tools in your mathematical "toolbox." You‚Äôve been working on **Data Fitting** (finding a line that goes *near* points), but the **Bisection Method** belongs to a different world called **Root Finding**.

To understand the difference, let‚Äôs look at the intuition and how it compares to what you already know.

---

### 1. Closed Methods (The "Bracketing" Strategy)

Closed methods are called "closed" because the root is **trapped** inside a known interval. You start with two boundaries, $a$ and $b$, and you know for a fact that the answer is somewhere between them.

- **Examples:** Bisection Method, Regula Falsi.

- **The Intuition:** It‚Äôs like a cage. You keep moving the walls of the cage closer together until the "animal" (the root) has nowhere to move.

- **Reliability:** These methods are **guaranteed to work**. Because the root is trapped, the method cannot "get lost." It will eventually find the answer, even if it takes a long time.

- **The Constraint:** You **must** find two points where the function changes sign ($+/-$) before you can even start.

where f(a) and f(b) have opposite signs, highlighting the root trapped between them]

---

### 2. Open Methods (The "Trajectory" Strategy)

Open methods do not "trap" the root. Instead, they start at one or two points and use the **local shape** of the curve to predict where the root might be.

- **Examples:** Newton-Raphson, Secant Method, Fixed-Point Iteration.

- **The Intuition:** It‚Äôs like a "Guided Missile" or a "Leap of Faith." You stand at a point, look at the slope, and jump in the direction you think the root is.

- **Reliability:** These methods are **risky**. They can "diverge," meaning they might jump further and further away from the root until your computer crashes or returns an error.

- **The Reward:** They are **incredibly fast**. While a closed method might take 50 steps to be perfect, an open method often finishes in 3 to 5 steps.

- **The Constraint:** You need more than just the sign; you usually need the **Derivative** (the slope) or at least two points to project a path.

| **Feature**          | **Closed Methods (Bracketing)**  | **Open Methods**                       |
| -------------------- | -------------------------------- | -------------------------------------- |
| **Starting Points**  | 2 points (must bracket the root) | 1 or 2 points (anywhere near the root) |
| **Convergence**      | Always (Guaranteed)              | Fast (but can fail/diverge)            |
| **Knowledge Needed** | Only the sign of $f(x)$          | Usually $f(x)$ and $f'(x)$ (slope)     |
| **Best Used For**    | When you need a "Safe" result    | When you need "High Speed" performance |

---

### 1. The Intuition of Bisection: The "High-Low" Game

Imagine I‚Äôm thinking of a number between 1 and 100.

- You guess **50**. I say "Higher."

- You now know the number is between **50 and 100**. You guess **75**.

- I say "Lower." Now you know it's between **50 and 75**.

**The Bisection Method is exactly this game.** If you have a continuous function $f(x)$, and you find one $x$ where the result is **negative** and another $x$ where it is **positive**, you *know* the function must have crossed **zero** somewhere in between.

**The Logic:**

1. **Pick an interval $[a, b]$** where $f(a)$ and $f(b)$ have opposite signs.

2. **Cut it in half** at the midpoint $c$.

3. Check the sign of $f(c)$.

4. Throw away the half that doesn't contain the zero and repeat.

5. **Result:** You "trap" the zero in a smaller and smaller box until you are accurate enough.

---

### 2. Why are we "Considering the Roots"?

In your previous project (Least Squares), the "Root" wasn't the goal. The **Line** was the goal.

In engineering, a **Root** ($f(x) = 0$) represents a **Solution to a Problem**:

- **Cybersecurity:** Finding the exact time a buffer will overflow.

- **Physics:** Finding the exact angle where a projectile hits a target.

- **Optimization:** Remember when we set the derivative to zero ($\frac{df}{dx} = 0$) to find the best-fitting line? **That is a root-finding problem.** We look for roots because $0$ is the "balance point" where an equation is solved.

---

### 3. Bisection vs. Everything Else (The Toolbox)

This is where students often get confused. Let‚Äôs distinguish between **Curve Fitting**, **Interpolation**, and **Root Finding**.

#### A. Data Fitting (Least Squares) - *What you just did*

- **Goal:** Find a trend for **messy, noisy data**.

- **Behavior:** The line doesn't have to hit every point; it just has to be "close" on average.

#### B. Interpolation (Splines, Newton Divided Difference)

- **Goal:** Create a smooth curve that **hits every single point** perfectly.

- **Difference:** You use this when your data is 100% accurate (like map coordinates) and you need to find values *between* the points you already have.

[Image comparing linear regression vs spline interpolation vs root finding]

#### C. Root Finding (Bisection, Newton-Raphson)

- **Goal:** You already have an equation; you just need to find the **$x$ that makes it true**.

- **Difference:** You aren't building a model from points; you are solving a model you already built.

| **Method**        | **Category**  | **Input**    | **Output**                        |
| ----------------- | ------------- | ------------ | --------------------------------- |
| **Least Squares** | Curve Fitting | Noisy Points | A Trend Equation ($y = mx+c$)     |
| **Splines / NDD** | Interpolation | Exact Points | A curve through every point       |
| **Bisection**     | Root Finding  | An Equation  | A single value $x$ where $f(x)=0$ |

---

### 4. Why learn Bisection if we have Least Squares?

Think of it as a chain:

1. You use **Least Squares** to find the equation of a curved trend ($y = ax^2 + bx + c$).

2. Now you want to know: *"At what time ($x$) does this trend reach zero?"*

3. You use the **Bisection Method** on that equation to find the answer.

---

### Summary

- **Interpolation (Splines/NDD)** is about **Connecting the Dots**.

- **Least Squares** is about **Ignoring the Noise** to see the trend.

- **Bisection** is about **Solving the Equation** once you have it.

**Does this help clarify why Bisection feels different? It‚Äôs a "Solver," while the others are "Model Builders."**

---

I'm glad that clicked! To really master the **Bisection Method**, you have to stop seeing it as a formula and start seeing it as a **Search Strategy**.

In computer science, we call this a **Binary Search**. It is the most robust, "never-fail" way to find a specific point.

---

### 1. The Analogy: The "Broken Wire" in the Dark

Imagine you have a 100-mile-long underground telegraph wire running from City A to City B. Somewhere in the middle, the wire is snapped. You need to find the exact location of the break, but you don't want to dig up the whole 100 miles.

1. **City A (Point $a$):** You test the signal. It‚Äôs "Live" (Positive).

2. **City B (Point $b$):** You test the signal. It‚Äôs "Dead" (Negative).

3. **The Intuition:** Since it‚Äôs Live at one end and Dead at the other, the break **must** be somewhere in between.

4. **The Move:** You drive to the 50-mile mark (the **Midpoint**) and dig a small hole to test the wire.
   
   - **Scenario A:** If the wire is "Live" at 50 miles, the break is between 50 and 100.
   
   - **Scenario B:** If the wire is "Dead" at 50 miles, the break is between 0 and 50.

5. **The Repeat:** You throw away the "good" half of the wire and repeat the process at the 25 or 75-mile mark.

---

### 2. The Theoretical "Must": Intermediate Value Theorem

The Bisection method relies on one simple rule from Calculus called the **Intermediate Value Theorem (IVT)**.

It basically says: *If a continuous function goes from a negative value to a positive value, it is physically impossible to get there without crossing the zero line.*

If your function is "discontinuous" (like a teleporting ghost that jumps over the x-axis), Bisection will fail. But for physics, engineering, and data fitting, our curves are almost always smooth and continuous.

---

### 3. The "Why": Why not just use Algebra?

You might ask: *"If I have $x^2 - 4 = 0$, I know $x=2$. Why do I need a search party?"*

In the real world, equations look like this:

$$f(x) = e^x - \sin(x) - x^2 = 0$$

There is **no simple algebra** to solve that for $x$. You can't just "isolate $x$." In these cases, your only option is to use a numerical "searcher" like Bisection to find the answer.

---

### 4. The Anatomy of the Algorithm (The 3 Questions)

When you write this in C, the program asks the same three questions in a loop:

1. **Where is the middle?** $c = (a + b) / 2$

2. **Is this the root?** If $f(c)$ is close enough to zero (e.g., $0.00001$), we stop!

3. **Which way do I go?** * If $f(c)$ has the same sign as $f(a)$, then the root isn't in that half. Move your "Left Wall" ($a$) to $c$.
   
   - Otherwise, move your "Right Wall" ($b$) to $c$.

---

### 5. The Strength and the Weakness

**The Strength (Reliability):** Bisection is like a "Tank." It is slow, but it **always** finds the root as long as your initial guesses ($a$ and $b$) bracket the zero. Unlike more advanced methods (like Newton-Raphson), it never gets "lost" or shoots off to infinity.

**The Weakness (Speed):**

Because it always cuts the interval exactly in half, it takes a predictable amount of time. To get one more decimal place of accuracy, you usually need about 3 to 4 more "digs" (iterations).

---

### Comparison Summary

- **Least Squares:** Found the "Pattern" of the wire.

- **Bisection:** Finds the "Break" in the wire.

- **The Link:** We often use Bisection **on** the results of our Least Squares to find where our model predicts a "Zero-Crossing" (like when a company will run out of money or when a signal will hit a threshold).

**Does the "Telegraph Wire" analogy make the "Root-Finding" process feel more like a physical search than an abstract math problem?**

---

If the **Bisection Method** is a "blind search" that only cares about signs, the **Regula Falsi** (False Position) method is a "smart search" that uses a bit of geometry to get to the answer faster.

### 1. The Analogy: The "Tug-of-War"

Imagine you are trying to find the exact point on a rope where the tension is zero.

- On the **Left ($a$)**, a very small child is pulling with a force of **-2**.

- On the **Right ($b$)**, a massive professional athlete is pulling with a force of **+100**.

**The Bisection Approach:** Bisection says, "I don't care who is pulling harder, just check the exact middle of the rope."

**The Regula Falsi Approach:** Regula Falsi says, "Wait! The athlete is much stronger than the child. The 'zero point' is probably much closer to the child than the athlete."

Instead of cutting the rope in half, Regula Falsi **draws a straight line** between the two strengths and guesses that the root is where that line hits the floor.

---

### 2. The Intuition: "Linear Interpolation"

The name "Regula Falsi" is Latin for "Rule of False Position." You are essentially making a **False Assumption**: you assume the function is a straight line between your two points.

1. You have two points: $(a, f(a))$ and $(b, f(b))$.

2. You connect them with a straight line (a **Secant line**).

3. You find where that line crosses the x-axis. This is your new guess, $c$.

4. Just like Bisection, you check the sign of $f(c)$ and replace the boundary that matches its sign.

---

### 3. The Formula (The "Slope" Logic)

You don't need to memorize a scary formula if you understand similar triangles. The "Smart Midpoint" $c$ is calculated as:

$$c = \frac{a \cdot f(b) - b \cdot f(a)}{f(b) - f(a)}$$

**Look at the logic:** It‚Äôs a weighted average. If $|f(b)|$ is much larger than $|f(a)|$, the formula pulls the guess $c$ closer to $a$.

---

### 4. Regula Falsi vs. Bisection

| **Feature**     | **Bisection**                  | **Regula Falsi**                           |
| --------------- | ------------------------------ | ------------------------------------------ |
| **Logic**       | Interval Halving               | Linear Approximation                       |
| **Speed**       | Slow but steady                | Usually faster (if the curve is flat)      |
| **Reliability** | Guaranteed to converge         | Guaranteed to converge                     |
| **Weakness**    | Ignores the "Weight" of values | Can get "stuck" if the curve is very sharp |

[Image comparing Bisection and Regula Falsi convergence on a curved function]

---

### 5. Why use this in C?

In your C programs, Regula Falsi is often preferred when the function is expensive to calculate (like a complex simulation). Because it uses the *values* of the function to make an educated guess, it often reaches the "Zero" in fewer steps than Bisection.

**The "Cyber" Use Case:**

Imagine you are trying to find the "Time to Failure" for a server under a DDoS attack. The load is increasing.

- At 1 minute, load is **10%**.

- At 5 minutes, load is **90%**.
  
  Regula Falsi will immediately "guess" that the 100% crash point is very close to the 5-minute mark, whereas Bisection would blindly check the 3-minute mark first.

---

### Summary

- **Bisection** is a "Dumb Search" (only looks at + or -).

- **Regula Falsi** is a "Smart Search" (looks at the actual values to tilt the guess).

**Does the idea of the "Tug-of-War" help you see why we use the values of $f(a)$ and $f(b)$ to "lean" our guess one way or the other?**

---

I understand. Sometimes analogies can feel like a "story" that doesn't quite explain the math. Let‚Äôs strip away the "Tug-of-War" and look at the **Geometric Intuition**‚Äîthe "Why" of the shape.

### 1. The Geometry: "Connecting the Dots"

In **Bisection**, you behave like you are blind to the curve. You only know if you are above or below the line.

In **Regula Falsi**, you use the information you have to create a **temporary shortcut**.

Imagine you have two points on a curve:

- Point **A** is way down in the basement ($y = -100$).

- Point **B** is just barely above the floor ($y = 1$).

**The Intuition:**

If the function were a straight line, where would it cross the floor? Common sense tells you it must cross **very close to B**, because B is already so close to the floor.

Regula Falsi mathematically "draws" that straight line between A and B and says: *"I know the real curve isn't a straight line, but this straight line is a much better guess than just picking the middle point."*

---

### 2. The "Weighted Average" Intuition

Think of Regula Falsi as a **Weighted Average** of your two boundaries ($a$ and $b$).

- In **Bisection**, the weight is always 50/50. (The middle).

- In **Regula Falsi**, the "weight" is the value of the function.

If $f(b)$ is **small** and $f(a)$ is **huge**, the formula "pulls" the next guess toward $b$. It‚Äôs like a magnet. The closer a point is to the x-axis ($y=0$), the more "pull" it has on our next guess.

---

### 3. Why is it called "False Position"?

This is the key to the intuition.

We are **falsely** assuming that between point A and point B, the world is a straight line.

- **Step 1:** Assume it's a line.

- **Step 2:** Find the zero of that "fake" line (this is easy algebra).

- **Step 3:** See where that puts us on the **real** curve.

- **Step 4:** Repeat.

By using a "Fake Line" that gets updated every step, we "hug" the curve as we move toward the root, rather than just jumping to the middle over and over.

---

### 4. Comparison of the "Movement"

- **Bisection** moves like a **Sledgehammer**: It chops the interval in half, no matter what. It is reliable but clumsy.

- **Regula Falsi** moves like a **Sniper**: It looks at the "slope" and aims for the zero. If the curve is relatively flat, it hits the target almost instantly.

[Image comparing the convergence of Bisection method and Regula Falsi method on a graph]

---

### 5. Why does it sometimes feel "Stuck"? (The Deep Intuition)

Here is one part of the intuition that most people miss:

If a curve is very "bent" (like a sharp "J" shape), one of the points ($a$ or $b$) might stay the same for a long time while the other point slowly creeps toward the root.

This happens because the "straight line" shortcut always misses on the same side of the curve. This is the only time Regula Falsi is slower than Bisection.

---

### Let's Summarize the Intuition

1. **Look at the heights:** Is one side much closer to zero than the other?

2. **Aim:** Point your next guess toward the "shorter" side.

3. **Update:** See how close you got, and draw a new line.

**Does thinking of it as a "Smart Aim" based on how close you are to the floor help the intuition click better than the tug-of-war did?**

---

**Next Step:** Would you like to see the **one-line formula** that actually calculates this "smart aim" in C? We can break down why the formula looks the way it does.

Let's break down the **Regula Falsi** (False Position) method into a clear, step-by-step "algorithm of the mind."

The goal here is to see how we go from two points on a curve to a single "smart guess" using the math of a straight line.

---

### Step 1: The Setup (The Bracket)

Just like Bisection, you start with two points, $a$ and $b$, where the function has different signs.

- $f(a)$ is negative (below the x-axis).

- $f(b)$ is positive (above the x-axis).

### Step 2: The "False" Assumption (The Secant Line)

Instead of finding the midpoint, we pretend the curve is a **straight line** connecting $(a, f(a))$ and $(b, f(b))$. In geometry, this is called a **Secant Line**.

### Step 3: Finding the "Smart Guess" ($c$)

We want to find the exact $x$ where this straight line hits zero. We use the property of **Similar Triangles** or the **Point-Slope form** of a line.

**The Breakdown of the Formula:**

The "Smart Guess" $c$ is calculated as:

$$c = b - \frac{f(b) \cdot (b - a)}{f(b) - f(a)}$$

**The Intuition of the Math:**

- $(b - a)$ is the total width of your search area.

- $f(b) / (f(b) - f(a))$ is a **percentage**. It tells you what fraction of the distance $(b - a)$ you need to move to hit the zero-crossing.

- If $f(b)$ is very small (close to zero), that fraction is small, and $c$ stays very close to $b$.

### Step 4: The Reality Check

Now that we have our "Smart Guess" $c$, we calculate the **actual** value of the function at that point: $f(c)$.

- If $f(c)$ is practically zero, **Success!** You found the root.

- If $f(c)$ is not zero, it will be either positive or negative.

### Step 5: The Update (The "Shrink")

This is exactly like Bisection:

- If $f(c)$ has the same sign as $f(a)$, then $c$ becomes the new $a$.

- If $f(c)$ has the same sign as $f(b)$, then $c$ becomes the new $b$.

---

### Why this is "Smart" (The Numerical Reason)

Imagine you are looking for a root near $x=10$.

- Your range is $a=0$ and $b=10$.

- $f(0) = -1,000,000$ (Way down in the deep ocean).

- $f(10) = 1$ (Just above the surface).

**Bisection** would guess $x=5$. It blindly cuts the distance in half.

**Regula Falsi** sees that $f(10)$ is much closer to zero than $f(0)$. The formula will calculate a $c$ that is approximately $9.999$.

It basically says: *"Don't waste time checking 5, 6, or 7. Based on the slope, the answer is almost certainly right next to 10."*

---

### The "Sticking Point" (When it's slow)

There is one scenario where Regula Falsi is annoying. If the curve is shaped like a **"U"** (concave), one side of the bracket might get "stuck."

1. The line always hits the x-axis on the same side of the root.

2. One endpoint (say $a$) stays exactly where it is for 20 iterations.

3. The other endpoint ($b$) creeps closer and closer, but never quite reaches it as fast as you'd like.

---

### Summary Checklist for your C Program:

1. **Input:** $a, b,$ and the function $f(x)$.

2. **Verify:** Check if $f(a) \cdot f(b) < 0$.

3. **Loop:**
   
   - Calculate $c$ using the "Slope Formula."
   
   - Calculate $f(c)$.
   
   - Replace the boundary that has the same sign as $f(c)$.

4. **Exit:** Stop when $f(c)$ is smaller than your tolerance (e.g., $10^{-6}$).

**Does this step-by-step breakdown make the transition from "drawing a line" to "updating the bracket" clear?**

---

[False Position Method | Numerical Methods - YouTube](https://www.youtube.com/watch?v=4217Y6OkCSk)

If Bisection is a **"Blind Search"** and Regula Falsi is a **"Smart Search,"** then **Newton-Raphson** is a **"Guided Missile."**

It is the fastest and most popular root-finding method in engineering because it doesn't just look at where you are; it looks at **which way you are headed**.

---

### 1. The Intuition: "Follow the Slope"

Imagine you are standing on a foggy mountainside. You want to find the "valley" (where $y=0$), but you can only see the ground beneath your feet.

- **Your Logic:** You feel the steepness of the ground where you are standing. You assume that if you follow that exact slope downward in a straight line, it will lead you to the valley floor.

- **The Action:** You walk down that slope until you hit the "predicted" floor.

- **The Reality:** Once you get there, the mountain has curved! You aren't at the valley yet. So, you feel the slope at your *new* location and repeat the process.

---

### 2. The Key Tool: The Tangent Line

Unlike the previous methods, Newton-Raphson only needs **one starting point ($x_0$)**, not a bracket of two.

1. **Start** at $x_0$.

2. **Calculate the Tangent:** Find the slope of the curve at that exact point. (In Calculus, this is the derivative, $f'(x)$).

3. **Project:** Follow that tangent line down until it hits the x-axis ($y=0$).

4. **Update:** That intersection point becomes your next guess ($x_1$).

---

### 3. The Formula Breakdown

The math for this is surprisingly simple. To find the next $x$, you take your current $x$ and subtract a "correction factor":

$$x_{next} = x_{current} - \frac{f(x_{current})}{f'(x_{current})}$$

**Why does this work?**

- $f(x)$ is **how far you are** from the goal (the height).

- $f'(x)$ is **how fast you are getting there** (the slope).

- If the slope is very steep, the correction is small (you‚Äôre almost there). If the slope is very flat, the correction is huge (you have a long way to go).

---

### 4. Why is it so fast? (Quadratic Convergence)

Newton-Raphson is famous for "Quadratic Convergence."

- **Bisection** might take 10 steps to gain 3 decimal places of accuracy.

- **Newton-Raphson** typically **doubles** the number of correct decimal places every single step.
  
  - If you have 2 correct digits, the next step gives you 4. The next gives you 8. It "snaps" to the answer.

---

### 5. The "Danger" (When the Missile Misses)

Because Newton-Raphson doesn't use a "bracket" (a floor and a ceiling), it can get lost.

- **Flat Spots:** If you hit a point where the slope is zero ($f'(x) = 0$), the formula tries to divide by zero. Your "missile" shoots off to infinity.

- **Infinite Loops:** On some weird curves, the method can bounce back and forth between two points forever.

- **Bad Starting Point:** If your $x_0$ is too far away, it might find a root on the other side of the graph that you didn't want.

[Image showing Newton-Raphson failure cases like local minima/maxima and divergence]

---

### 6. Comparison: The "Three Seekers"

| **Method**         | **Requirements**         | **Logic**             | **Speed**                  |
| ------------------ | ------------------------ | --------------------- | -------------------------- |
| **Bisection**      | 2 points (bracket)       | Half-way point        | Slow but "Unkillable"      |
| **Regula Falsi**   | 2 points (bracket)       | String between points | Moderate / Reliable        |
| **Newton-Raphson** | **1 point + Derivative** | **Tangent line**      | **Lightning Fast / Risky** |

---

### How to use it in C

When you write this in C, you usually need two functions:

1. `f(x)`: The original equation.

2. `df(x)`: The derivative of that equation.

C

```
// One iteration of Newton-Raphson
double next_x = current_x - (f(current_x) / df(current_x));
```

---

Since Newton-Raphson relies on the **derivative**, we need to move from "Static Algebra" to "Motion." In C programming, this means we need a way to calculate the slope at any given point.

### 1. The Derivative: The "Speedometer"

For your quadratic curve $f(x) = ax^2 + bx + c$, the derivative $f'(x)$ tells us the slope at any point $x$.

Using the power rule from calculus:

- The derivative of $ax^2$ is **$2ax$**.

- The derivative of $bx$ is **$b$**.

- The derivative of the constant $c$ is **$0$**.

So, your "Slope Function" is:

$$f'(x) = 2ax + b$$

---

### 2. The Step-by-Step Execution

Let's see exactly how the "Guided Missile" hits the target in 3 steps. Imagine we have the equation $f(x) = x^2 - 4$. We know the root is **2**. Let‚Äôs start with a guess of **$x_0 = 3$**.

**Step 1:**

- Current Guess: $3$

- Height $f(3) = 3^2 - 4 = \mathbf{5}$

- Slope $f'(3) = 2(3) = \mathbf{6}$

- **Next Guess:** $x_1 = 3 - (5 / 6) = \mathbf{2.166}$

**Step 2:**

- Current Guess: $2.166$

- Height $f(2.166) = 0.694$

- Slope $f'(2.166) = 4.332$

- **Next Guess:** $x_2 = 2.166 - (0.694 / 4.332) = \mathbf{2.006}$

**Step 3:**

- Current Guess: $2.006$

- Height $f(2.006) = 0.024$

- Slope $f'(2.006) = 4.012$

- **Next Guess:** $x_3 = 2.006 - (0.024 / 4.012) = \mathbf{2.00003}$

**In just 3 steps, we went from "3" to "2.00003".** Bisection would have taken dozens of steps to get that close!

---

### 3. Implementing the "Missile" in C

To make this work in your C workflow, you would define two functions. This is much cleaner than the "bracket" logic of Bisection.

C

```
// The original function (The Mountain)
double f(double x, double a, double b, double c) {
    return a*x*x + b*x + c;
}

// The derivative (The Slope)
double df(double x, double a, double b) {
    return 2*a*x + b;
}

// The Newton-Raphson Engine
double solve_newton(double guess, double a, double b, double c) {
    double x = guess;
    for(int i = 0; i < 10; i++) { // Run for 10 iterations
        double slope = df(x, a, b);
        if (slope == 0) break; // Avoid dividing by zero!

        x = x - (f(x, a, b, c) / slope);
    }
    return x;
}
```

---

### 4. The "Intuition Check": Why the division?

Look at the formula again: $x - \frac{f(x)}{f'(x)}$.

- If $f(x)$ is the **Distance** to the floor.

- And $f'(x)$ is the **Speed** toward the floor.

- Then $\frac{\text{Distance}}{\text{Speed}} = \text{Time}$ (or in this case, the **Step Size**).

Newton-Raphson is simply saying: *"Based on how fast I'm descending right now, I should take a step of exactly this size to hit the floor."*

---

### 5. Final Comparison: Which one should you use?

1. **Use Bisection** if you have no idea what the curve looks like and you just need a "safe" answer.

2. **Use Regula Falsi** if you want something faster than Bisection but you don't know the derivative.

3. **Use Newton-Raphson** if you have the derivative and a decent starting guess. It is the "Professional's Choice."

---

When we talk about a **System of Non-Linear Equations**, we are moving from finding a single "root" on a flat line to finding the **intersection points** of multiple curved surfaces in space.

If a single non-linear equation is like finding where a mountain path crosses a specific altitude, a system of non-linear equations is like finding the exact spot where **two different mountain ranges collide.**

---

### 1. The Intuition: The "Meeting Point"

In a linear system ($2x + 3y = 5$), everything is a straight line. They can only cross **once**.

In a **Non-Linear System**, the "lines" can be circles, parabolas, or waves.

- **Example:**
  
  1. $x^2 + y^2 = 25$ (A Circle)
  
  2. $y - x^2 = -5$ (A Parabola)

**The Goal:** Find the $(x, y)$ pairs that satisfy **both** equations at the same time. Unlike linear systems, these can have no solutions, one solution, or many solutions.

---

### 2. Why are these so hard to solve?

In linear algebra, we have a "Master Key" (Matrix Inversion/Cramer's Rule). For non-linear systems, there is **no general formula**.

You cannot simply "substitute and solve" because the variables are tangled up in powers ($x^2$), roots ($\sqrt{x}$), or trigonometry ($\sin(x)$). To solve them, we have to use the "Open Methods" you just learned, but "upgraded" to multiple dimensions.

---

### 3. The "Guided Missile" Upgrade: Newton-Raphson for Systems

Remember how Newton-Raphson used a slope ($f'(x)$) to find a root? For a system of equations, we use a **Matrix of Slopes** called the **Jacobian Matrix**.

#### The Concept:

1. **Pick a starting point** $(x_0, y_0)$.

2. **Calculate the Jacobian:** This matrix tells you how *every* equation changes with respect to *every* variable. It‚Äôs like a 360-degree compass.

3. **Solve a Linear Step:** We pretend the curves are flat "planes" for a split second. We solve a mini-linear system to see which way to jump.

4. **Jump:** Move to a new $(x, y)$ and repeat.

---

### 4. Real-World Use Case: GPS Satellites

This is the most famous use of a non-linear system.

- Your phone receives signals from 4 different satellites.

- Each signal gives you an equation for a **sphere** (representing the distance from that satellite).

- **The System:** You need to find the one $(x, y, z)$ point where all four spheres intersect.

- **The Solver:** Your phone's chip runs a version of the **Newton-Raphson system solver** hundreds of times a second to find your location.

---

### 5. Summary: Linear vs. Non-Linear Systems

| **Feature**         | **Linear System**        | **Non-Linear System**                  |
| ------------------- | ------------------------ | -------------------------------------- |
| **Visual**          | Straight lines crossing  | Curves, circles, and waves crossing    |
| **Number of Roots** | Exactly 1 (usually)      | 0, 1, or many                          |
| **Primary Tool**    | Matrix Solver (Cramer's) | Newton-Raphson + Jacobian              |
| **Complexity**      | Simple, direct           | Iterative, needs a good starting guess |

---
